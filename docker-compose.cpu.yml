# CPU Override for slow inference
# ================================
# Usage: docker compose -f docker-compose.yml -f docker-compose.cpu.yml up
# Or via run.sh: ./scripts/run.sh --cpu -m MODEL -k KV -d DATASET
#
# NOTE: API timeout is now configured in config/mini-swe-agent/swebench.yaml
# via model_kwargs.timeout (default: 7200s = 2 hours).
#
# The previous HTTPX_TIMEOUT, OPENAI_TIMEOUT, and LITELLM_REQUEST_TIMEOUT
# environment variables did nothing - they are not recognized by httpx,
# openai-python, or litellm SDK. The timeout must be passed as a parameter
# to the OpenAI client, which litellm does via the 'timeout' kwarg.

services:
  swe-agent:
    # No additional overrides needed - timeout is in config file
    # This file is kept for potential future CPU-specific settings
    environment: []
