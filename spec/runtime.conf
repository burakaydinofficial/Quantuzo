# Runtime Configuration
# =====================
# Base settings for all benchmark runs.
# Model configs can override these values.

# Context size per slot (each conversation gets this much context)
# Total allocated = CTX_SIZE * PARALLEL
# Note: Results with different context sizes are not directly comparable
CTX_SIZE=65536

# CPU thread settings (adjust based on your system)
THREADS=16
THREADS_BATCH=8

# Evaluation parallelism
EVAL_WORKERS=8

# llama-server parallel slots (concurrent request processing)
# Each slot allocates full CTX_SIZE of KV cache memory
PARALLEL=1

# SWE-agent parallel workers (concurrent instance processing)
# Higher values = more instances processed simultaneously
# Requires sufficient model server capacity (PARALLEL slots)
WORKERS=1
