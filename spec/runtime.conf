# Runtime Configuration
# =====================
# Base settings for all benchmark runs.
# Model configs can override these values.

# Context size per slot (each conversation gets this much context)
# Total allocated = CTX_SIZE * PARALLEL
# Note: Results with different context sizes are not directly comparable
CTX_SIZE=65536

# CPU thread settings (adjust based on your system)
THREADS=16
THREADS_BATCH=8

# Evaluation parallelism
EVAL_WORKERS=8

# llama-server parallel slots (concurrent request processing)
# Each slot allocates full CTX_SIZE of KV cache memory
PARALLEL=1

# SWE-agent parallel workers (concurrent instance processing)
# Higher values = more instances processed simultaneously
# Requires sufficient model server capacity (PARALLEL slots)
WORKERS=1

# Flash attention mode for GPU inference
# Options: auto, on, off
# - auto: detect hardware support (may fail with some KV cache types)
# - on: force enable (requires full GPU offload, no partial CPU)
# - off: disable (needed for partial GPU offload or older GPUs)
FLASH_ATTN=auto

# Extra arguments for llama-server
# Example: EXTRA_LLAMA_ARGS="-nkvo" to offload KV cache to CPU
EXTRA_LLAMA_ARGS=
