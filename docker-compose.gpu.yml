# GPU/CUDA Override for llama-server
# ===================================
# Usage: docker compose -f docker-compose.yml -f docker-compose.gpu.yml up
# Or via run.sh: ./scripts/run.sh --gpu -m MODEL -k KV -d DATASET
#
# Prerequisites:
#   - NVIDIA GPU with CUDA support
#   - nvidia-container-toolkit installed on host
#   - Docker configured with nvidia runtime
#
# Install nvidia-container-toolkit (Ubuntu/Debian):
#   curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
#   curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
#     sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
#     sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
#   sudo apt-get update && sudo apt-get install -y nvidia-container-toolkit
#   sudo nvidia-ctk runtime configure --runtime=docker
#   sudo systemctl restart docker

services:
  llama-server:
    # Use official pre-built CUDA image (no local build needed)
    # When both image and build are present, Compose pulls the image first
    image: ghcr.io/ggml-org/llama.cpp:server-cuda

    # GPU device access
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

    # Override command for GPU mode
    # - Removes CPU-specific flags (threads)
    # - Adds GPU layer offloading
    command: >
      --model /models/${MODEL_FILE:-model.gguf}
      --ctx-size ${CTX_SIZE:-32768}
      --cache-type-k ${KV_TYPE_K:-f16}
      --cache-type-v ${KV_TYPE_V:-f16}
      --n-gpu-layers 999
      --flash-attn
      --host 0.0.0.0
      --port 8080
